{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f39051-ee36-48c9-9ab2-0824cc65d83c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4747c2bb-294c-496b-a8fc-d282f18178c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume DataFrame columns: ['Unnamed: 0', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'responsibilities', '\\ufeffjob_position_name']\n",
      "Job DataFrame columns: ['Unnamed: 0', 'job_id', 'skill_abr', 'skill_name', 'industry_id', 'industry_name', 'title', 'description']\n",
      "Resume DataFrame columns after cleaning: ['Unnamed: 0', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'responsibilities', 'job_position_name']\n",
      "Job DataFrame columns after cleaning: ['Unnamed: 0', 'job_id', 'skill_abr', 'skill_name', 'industry_id', 'industry_name', 'title', 'description']\n"
     ]
    }
   ],
   "source": [
    "resumes_df = pd.read_csv('cleaned_resume.csv')\n",
    "jobs_df = pd.read_csv('cleaned_job.csv')\n",
    "\n",
    "print(\"Resume DataFrame columns:\", resumes_df.columns.tolist())\n",
    "print(\"Job DataFrame columns:\", jobs_df.columns.tolist())\n",
    "\n",
    "resumes_df.columns = [col.replace('\\ufeff', '') for col in resumes_df.columns]\n",
    "jobs_df.columns = [col.replace('\\ufeff', '') for col in jobs_df.columns]\n",
    "\n",
    "# Clean column names by removing leading/trailing spaces\n",
    "resumes_df.columns = resumes_df.columns.str.strip()\n",
    "jobs_df.columns = jobs_df.columns.str.strip()\n",
    "\n",
    "# Verify column names after cleaning\n",
    "print(\"Resume DataFrame columns after cleaning:\", resumes_df.columns.tolist())\n",
    "print(\"Job DataFrame columns after cleaning:\", jobs_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9112de9-b18c-4ff2-b1da-30152724a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Clean data\n",
    "# 1. Handle missing values\n",
    "resumes_df = resumes_df.fillna('')\n",
    "jobs_df = jobs_df.fillna('')\n",
    "\n",
    "def safe_join(item):\n",
    "    if isinstance(item, list):\n",
    "        return ', '.join(item)\n",
    "    elif isinstance(item, str):\n",
    "        return item\n",
    "    elif item is None:\n",
    "        return ''\n",
    "    else:\n",
    "        return str(item)\n",
    "\n",
    "# 2. Create unified text representation for resumes\n",
    "resumes_df['resume_text'] = resumes_df.apply(\n",
    "    lambda row: f\"Skills: {safe_join(row['skills'])} \\n\"\n",
    "                f\"Education: {safe_join(row['educational_institution_name'])} - \"\n",
    "                f\"{safe_join(row['degree_names'])} - \"\n",
    "                f\"{safe_join(row['major_field_of_studies'])} \\n\"\n",
    "                f\"Work Experience: {safe_join(row['professional_company_names'])} - \"\n",
    "                f\"{safe_join(row['positions'])} \\n\"\n",
    "                f\"Responsibilities: {row['responsibilities']} \\n\"\n",
    "                f\"Target Position: {row['job_position_name']}\", axis=1)\n",
    "\n",
    "jobs_df['job_text'] = jobs_df.apply(\n",
    "    lambda row: f\"Job Title: {row['title']} \\n\"\n",
    "                f\"Required Skills: {safe_join(row['skill_name'])} \\n\"\n",
    "                f\"Industry: {safe_join(row['industry_name'])} \\n\"\n",
    "                f\"Job Description: {row['description']}\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f34480-7172-4850-889d-a4bcf49a7bc4",
   "metadata": {},
   "source": [
    "## Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc93e23-0cf1-47c3-8cba-23723fbc2f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c950523a07c1496ea24aedf512fa7e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd9d42b44c34ab98ece5b07a79c7b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfc54b52d4c4a3bbd27b69f8a1044ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e714671496d460b8cd8baf5322ec46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0694285fa813419abe9a2b55666e3baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f405e55623ef4426a476a096fd58a33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a897a3b46ff469e99a7b75ae7a5b9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edaff096ce84ff08f5a89b0c7dddacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b16499d3c74aa991be98dfb27f3bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca110234f4948c79be289e436e56b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda1cc089fcf415b8e40cc7b5838179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa3278bf1e246feac6cd3802e3f393e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use pre-trained models to convert resumes and job descriptions into vectors:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "resume_embeddings = model.encode(resumes_df['resume_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "job_embeddings = model.encode(jobs_df['job_text'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39486c7-f5c8-493d-890c-48a0a0b57bee",
   "metadata": {},
   "source": [
    "## Building Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bdcc33b-b32e-4bc7-9b69-5c33fae482ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.2/13.7 MB 29.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.7 MB 37.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 33.1 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b08a67cc-383b-4482-bb68-69b082fe013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use FAISS as vector databases:\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to float32 type\n",
    "resume_embeddings = np.array(resume_embeddings).astype('float32')\n",
    "job_embeddings = np.array(job_embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "resume_dimension = resume_embeddings.shape[1]\n",
    "resume_index = faiss.IndexFlatL2(resume_dimension)\n",
    "resume_index.add(resume_embeddings)\n",
    "\n",
    "job_dimension = job_embeddings.shape[1]\n",
    "job_index = faiss.IndexFlatL2(job_dimension)\n",
    "job_index.add(job_embeddings)\n",
    "\n",
    "faiss.write_index(resume_index, \"resume_index.faiss\")\n",
    "faiss.write_index(job_index, \"job_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc42ac-c18f-4cd9-967c-afc21b066434",
   "metadata": {},
   "source": [
    "## Similarity Calculation and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "af7741c1-dcf6-4bce-a899-a90938d3f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_safe_value(item, default_value):\n",
    "    if isinstance(item, list) and len(item) > 0:\n",
    "        return item[0]\n",
    "    elif isinstance(item, str) and item:\n",
    "        return item\n",
    "    else:\n",
    "        return default_value\n",
    "\n",
    "def find_matching_resumes(job_id, top_k=5):\n",
    "    \"\"\"Find resumes that best match a specific job\"\"\"\n",
    "    job_vector = job_embeddings[job_id].reshape(1, -1).astype('float32')\n",
    "    \n",
    "    distances, indices = resume_index.search(job_vector, top_k)\n",
    "    \n",
    "    matches = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        resume_vector = resume_embeddings[idx].reshape(1, -1)\n",
    "        similarity_score = cosine_similarity(job_vector, resume_vector)[0][0]\n",
    "        \n",
    "        company = resumes_df.iloc[idx]['professional_company_names']\n",
    "        position = resumes_df.iloc[idx]['positions']\n",
    "        \n",
    "        company_name = get_safe_value(company, \"Unknown Company\")\n",
    "        position_name = get_safe_value(position, \"Unknown Position\")\n",
    "        \n",
    "        candidate_identifier = f\"{company_name} - {position_name}\"\n",
    "        \n",
    "        matches.append({\n",
    "            'resume_id': idx,\n",
    "            'similarity': similarity_score,\n",
    "            'resume_text': resumes_df.iloc[idx]['resume_text'],\n",
    "            'candidate_identifier': candidate_identifier\n",
    "        })\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "def find_matching_jobs(resume_id, top_k=5):\n",
    "    \"\"\"Find jobs that best match a specific resume\"\"\"\n",
    "    resume_vector = resume_embeddings[resume_id].reshape(1, -1).astype('float32')\n",
    "    \n",
    "    distances, indices = job_index.search(resume_vector, top_k)\n",
    "    \n",
    "    matches = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        job_vector = job_embeddings[idx].reshape(1, -1)\n",
    "        similarity_score = cosine_similarity(resume_vector, job_vector)[0][0]\n",
    "        \n",
    "        # Use job titles and industries as job identifiers\n",
    "        title = jobs_df.iloc[idx]['title']\n",
    "        \n",
    "        industry = jobs_df.iloc[idx]['industry_name']\n",
    "        industry_name = get_safe_value(industry, \"Unknown Industry\")\n",
    "            \n",
    "        job_identifier = f\"{title} - {industry_name}\"\n",
    "        \n",
    "        matches.append({\n",
    "            'job_id': idx,\n",
    "            'similarity': similarity_score,\n",
    "            'job_title': title,\n",
    "            'job_identifier': job_identifier\n",
    "        })\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034cb45-9a8d-465e-9b9a-42fef8e4d0c9",
   "metadata": {},
   "source": [
    "## Enhanced Match Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a827a231-d303-44c7-ae1c-e49381d06724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-3778094c3c034b5bbc65218e7f65f6b3\",\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "def generate_match_explanation(resume_text, job_text, similarity_score):\n",
    "    \"\"\"Use LLM to generate match explanations\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Resume content:\n",
    "    {resume_text}\n",
    "    \n",
    "    Job description:\n",
    "    {job_text}\n",
    "    \n",
    "    Similarity score: {similarity_score:.2f}\n",
    "    \n",
    "    Please analyze the match between this resume and the job, indicating:\n",
    "    1. Which skills and experiences of the candidate match the job requirements\n",
    "    2. Key skills or experiences the candidate may be lacking\n",
    "    3. Suggestions for how the candidate could adjust their resume to better match this position\n",
    "    4. Provide a match rating from 1-10 with explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-reasoner\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional recruitment consultant who specializes in analyzing resume-job matches.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6d873-b2e6-4bca-b068-9adbf9be0dcb",
   "metadata": {},
   "source": [
    "## Building Retrieval Interface (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2101517a-a41c-40df-80a8-725a5fb523fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from fastapi) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from fastapi) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: uvicorn, starlette, fastapi\n",
      "Successfully installed fastapi-0.115.12 starlette-0.46.1 uvicorn-0.34.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "044e134f-9bd1-4dfc-95c0-268994932592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple API interface to query matching results:\n",
    "from fastapi import FastAPI, Query\n",
    "from typing import List, Optional\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/match/job/{job_id}\")\n",
    "def match_job(job_id: int, top_k: Optional[int] = 5, explain: Optional[bool] = False):\n",
    "    \"\"\"Find resumes matching a specific job\"\"\"\n",
    "    matches = find_matching_resumes(job_id, top_k)\n",
    "    \n",
    "    if explain and matches:\n",
    "        # Add explanation for each match\n",
    "        for match in matches:\n",
    "            job_text = jobs_df.iloc[job_id]['job_text']\n",
    "            resume_text = match['resume_text']\n",
    "            match['explanation'] = generate_match_explanation(\n",
    "                resume_text, job_text, match['similarity']\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        \"job\": jobs_df.iloc[job_id]['title'],\n",
    "        \"matches\": matches\n",
    "    }\n",
    "\n",
    "@app.get(\"/match/resume/{resume_id}\")\n",
    "def match_resume(resume_id: int, top_k: Optional[int] = 5, explain: Optional[bool] = False):\n",
    "    \"\"\"Find jobs matching a specific resume\"\"\"\n",
    "    matches = find_matching_jobs(resume_id, top_k)\n",
    "    \n",
    "    if explain and matches:\n",
    "        # Add explanation for each match\n",
    "        for match in matches:\n",
    "            resume_text = resumes_df.iloc[resume_id]['resume_text']\n",
    "            job_text = match['job_text'] = jobs_df.iloc[match['job_id']]['job_text']\n",
    "            match['explanation'] = generate_match_explanation(\n",
    "                resume_text, job_text, match['similarity']\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        \"candidate\": f\"Candidate_{resume_id}\",\n",
    "        \"matches\": matches\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a5def79-429b-4ae0-bd76-2126692af49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the API server, press Ctrl+C to stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5900]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [5900]\n"
     ]
    }
   ],
   "source": [
    "# Only used when API services are needed\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"Start the API server, press Ctrl+C to stop...\")\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae9723-4a5c-4cfb-a811-d1ec361a5916",
   "metadata": {},
   "source": [
    "## System Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d97cc05e-e50b-4a7a-ad6f-c0f24c27a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_matching_system(test_cases):\n",
    "    \"\"\"Evaluate matching system performance\"\"\"\n",
    "    precision_at_k = []\n",
    "    recall_at_k = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        job_id = test['job_id']\n",
    "        relevant_resumes = set(test['relevant_resume_ids'])\n",
    "        \n",
    "        # Get system recommended resumes\n",
    "        matches = find_matching_resumes(job_id, top_k=10)\n",
    "        recommended_resumes = set([m['resume_id'] for m in matches])\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = len(relevant_resumes.intersection(recommended_resumes)) / len(recommended_resumes)\n",
    "        recall = len(relevant_resumes.intersection(recommended_resumes)) / len(relevant_resumes)\n",
    "        \n",
    "        precision_at_k.append(precision)\n",
    "        recall_at_k.append(recall)\n",
    "    \n",
    "    return {\n",
    "        'avg_precision': sum(precision_at_k) / len(precision_at_k),\n",
    "        'avg_recall': sum(recall_at_k) / len(recall_at_k)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c8f4f-7844-4b98-a7c2-a1772de16283",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "900d5bd9-c217-4191-a76d-7aa8c9e7950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing Job-to-Resume Matching =====\n",
      "Matching resumes for job 'Marketing Coordinator':\n",
      "1. Similarity: 0.5708 - ['FreshDirect', 'Marketing Science Associates'] - ['Full Stack Engineer Intern', 'Full Stack Engineer Intern']\n",
      "2. Similarity: 0.5685 - ['Titli Foundation'] - ['System Developer']\n",
      "3. Similarity: 0.5761 - ['N/A'] - ['N/A']\n",
      "4. Similarity: 0.6001 - ['Flipkart'] - ['Intern']\n",
      "5. Similarity: 0.5826 - ['ASM Management'] - ['Intern']\n",
      "6. Similarity: 0.5394 - ['Titli Foundation'] - ['System Developer']\n",
      "7. Similarity: 0.5516 - ['Biswa Pvt Ltd'] - ['Part-Time Analyst']\n",
      "8. Similarity: 0.5443 - Unknown Company - Unknown Position\n",
      "9. Similarity: 0.5386 - ['RNT Laboratories'] - ['Intern Trainee']\n",
      "10. Similarity: 0.5678 - ['DKB Innovations'] - ['Python Intern']\n",
      "\n",
      "===== Testing Resume-to-Job Matching =====\n",
      "Matching jobs for resume #0:\n",
      "1. Similarity: 0.7597 - Big Data Developer - ['Information Services']\n",
      "2. Similarity: 0.7438 - Data Analyst: 24-00481 - ['IT Services and IT Consulting', 'Software Development', 'Financial Services']\n",
      "3. Similarity: 0.7476 - Senior Data Scientist - ['Engineering Services']\n",
      "4. Similarity: 0.7297 - Data Scientist - ['Information Technology & Services']\n",
      "5. Similarity: 0.7301 - Data Scientist - ['Computers and Electronics Manufacturing']\n",
      "6. Similarity: 0.7259 - Data Scientist - ['Human Resources Services']\n",
      "7. Similarity: 0.7239 - Entry Level Data Scientist - ['E-Learning Providers']\n",
      "8. Similarity: 0.7230 - Data Scientist - ['Pharmaceutical Manufacturing', 'Biotechnology Research', 'Research Services']\n",
      "9. Similarity: 0.7161 - NLP Data Scientist - Python | Machine Learning | AI - Hybrid - TS/SCI - Tampa or DC - ['Accounting', 'IT Services and IT Consulting', 'Business Consulting and Services']\n",
      "10. Similarity: 0.7249 - Software Developer - ['IT Services and IT Consulting']\n",
      "\n",
      "===== Testing Match Explanation =====\n",
      "Generating match explanation...\n",
      "Match explanation:\n",
      "### **1. Matching Skills and Experiences**  \n",
      "- **Marketing & Sales Exposure**: The candidate’s experience in Trade Marketing Executive roles includes responsibilities like brand visibility, campaign management, and sales targets, which align with the job’s focus on marketing coordination and agent support.  \n",
      "- **Project Management**: Skills like KPIs tracking, Excel, and campaign execution suggest organizational abilities relevant to managing marketing requests, deadlines, and event planning.  \n",
      "- **Team Collaboration**: Experience working with cross-functional teams (e.g., field marketing, product distribution) could translate to collaborating with sales agents and marketing teams.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. Key Gaps**  \n",
      "- **Graphic Design Proficiency**: The job heavily emphasizes Adobe Creative Cloud (InDesign, Illustrator, Photoshop) and custom design work, which are absent from the resume.  \n",
      "- **Real Estate/Event Planning Experience**: No demonstrated exposure to real estate marketing, event coordination, or vendor management.  \n",
      "- **Software Skills**: Microsoft Office is mentioned, but Adobe Suite is critical for this role.  \n",
      "- **Industry Relevance**: Background in hygiene products/tech (via internships) lacks direct alignment with real estate.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Resume Adjustments**  \n",
      "- **Highlight Transferable Marketing Skills**: Reframe responsibilities like “Campaigns” and “Brand Visibility” to emphasize creative execution, content creation, or design collaboration (even if basic).  \n",
      "- **Add Relevant Tools**: If the candidate has *any* familiarity with Adobe tools (e.g., from academic projects or personal use), include them.  \n",
      "- **Focus on Soft Skills**: Stress “cool-under-pressure,” teamwork, and organizational skills from managing campaigns and product distribution.  \n",
      "- **Tailor Job Titles**: Use “Trade Marketing Executive” as the headline (not “Full Stack Engineer Intern”) and downplay unrelated technical roles.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Match Rating: 3/10**  \n",
      "**Explanation**:  \n",
      "- The candidate’s marketing experience (e.g., campaigns, brand strategy) and soft skills (organization, teamwork) provide a minimal foundation. However, the lack of **graphic design expertise**, **Adobe Suite proficiency**, and **real estate/event planning exposure** are critical mismatches. The role prioritizes creativity, administrative coordination, and industry-specific tasks, which the resume does not substantiate. While the candidate could grow into the role with training, the current profile is a weak fit for the job’s core requirements.  \n",
      "\n",
      "**Recommendation**: The candidate should apply only if they have unlisted graphic design skills or are open to entry-level training. Otherwise, target roles in **brand management** or **trade marketing** within FMCG/retail, where their background is stronger.\n"
     ]
    }
   ],
   "source": [
    "# Test matching functionality\n",
    "if __name__ == \"__main__\":\n",
    "    sample_job_id = 0\n",
    "    sample_resume_id = 0\n",
    "    \n",
    "    print(\"===== Testing Job-to-Resume Matching =====\")\n",
    "    matching_resumes = find_matching_resumes(sample_job_id, top_k=10)\n",
    "    print(f\"Matching resumes for job '{jobs_df.iloc[sample_job_id]['title']}':\")\n",
    "    for i, match in enumerate(matching_resumes):\n",
    "        print(f\"{i+1}. Similarity: {match['similarity']:.4f} - {match['candidate_identifier']}\")\n",
    "    \n",
    "    print(\"\\n===== Testing Resume-to-Job Matching =====\")\n",
    "    matching_jobs = find_matching_jobs(sample_resume_id, top_k=10)\n",
    "    print(f\"Matching jobs for resume #{sample_resume_id}:\")\n",
    "    for i, match in enumerate(matching_jobs):\n",
    "        print(f\"{i+1}. Similarity: {match['similarity']:.4f} - {match['job_identifier']}\")\n",
    "    \n",
    "    print(\"\\n===== Testing Match Explanation =====\")\n",
    "    if matching_resumes:\n",
    "        job_text = jobs_df.iloc[sample_job_id]['job_text']\n",
    "        resume_text = matching_resumes[0]['resume_text']\n",
    "        similarity = matching_resumes[0]['similarity']\n",
    "        \n",
    "        print(\"Generating match explanation...\")\n",
    "        explanation = generate_match_explanation(resume_text, job_text, similarity)\n",
    "        print(\"Match explanation:\")\n",
    "        print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2101113-299d-4bcb-82c0-66bf17abc624",
   "metadata": {},
   "source": [
    "## About saving and next loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e4c6a7dd-dfd1-490c-a0e0-472e37754225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS indexes saved\n",
      "Embedding vectors saved\n",
      "Dataframes saved\n",
      "RAG system saving completed!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Save the Current RAG System\n",
    "# Save the complete RAG system\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Save FAISS indexes\n",
    "faiss.write_index(resume_index, \"resume_index.faiss\")\n",
    "faiss.write_index(job_index, \"job_index.faiss\")\n",
    "print(\"FAISS indexes saved\")\n",
    "\n",
    "# 2. Save embedding vectors\n",
    "np.save(\"resume_embeddings.npy\", resume_embeddings)\n",
    "np.save(\"job_embeddings.npy\", job_embeddings)\n",
    "print(\"Embedding vectors saved\")\n",
    "\n",
    "# 3. Save dataframes\n",
    "rag_data = {\n",
    "    \"resumes_df\": resumes_df,\n",
    "    \"jobs_df\": jobs_df\n",
    "}\n",
    "\n",
    "with open(\"rag_data.pkl\", 'wb') as f:\n",
    "    pickle.dump(rag_data, f)\n",
    "print(\"Dataframes saved\")\n",
    "\n",
    "print(\"RAG system saving completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072eaf12-c40c-4951-a3ed-bcd96359fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9544 resumes and 127125 jobs\n",
      "Embedding vectors loaded\n",
      "FAISS indexes loaded\n",
      "Embedding model loaded\n",
      "RAG system loading completed, ready to use matching functions!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the System Next Time\n",
    "# Load the previously saved RAG system\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load dataframes\n",
    "with open(\"rag_data.pkl\", 'rb') as f:\n",
    "    rag_data = pickle.load(f)\n",
    "\n",
    "resumes_df = rag_data[\"resumes_df\"]\n",
    "jobs_df = rag_data[\"jobs_df\"]\n",
    "print(f\"Loaded {len(resumes_df)} resumes and {len(jobs_df)} jobs\")\n",
    "\n",
    "# 2. Load embedding vectors\n",
    "resume_embeddings = np.load(\"resume_embeddings.npy\")\n",
    "job_embeddings = np.load(\"job_embeddings.npy\")\n",
    "print(\"Embedding vectors loaded\")\n",
    "\n",
    "# 3. Load FAISS indexes\n",
    "resume_index = faiss.read_index(\"resume_index.faiss\")\n",
    "job_index = faiss.read_index(\"job_index.faiss\")\n",
    "print(\"FAISS indexes loaded\")\n",
    "\n",
    "# 4. Load embedding model (if you need to create new embeddings)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded\")\n",
    "\n",
    "print(\"RAG system loading completed, ready to use matching functions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b630b39-9aeb-48ad-abb4-477d33d9366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the Matching Functions\n",
    "# e.g. Find resumes matching a specific job\n",
    "job_id = 1  # Replace with the job ID you want to query\n",
    "matching_resumes = find_matching_resumes(job_id, top_k=5)\n",
    "print(f\"Resumes matching job {job_id}:\")\n",
    "print(matching_resumes)\n",
    "\n",
    "# e.g. Find jobs matching a specific resume\n",
    "resume_id = 1  # Replace with the resume ID you want to query\n",
    "matching_jobs = find_matching_jobs(resume_id, top_k=5)\n",
    "print(f\"Jobs matching resume {resume_id}:\")\n",
    "print(matching_jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
